\chapter{Introduction}
\label{chap:introduction}

The proliferation of cloud-based services poses new opportunities and
challenges for hosting data.  On the one hand, the availability of
professionally-maintained infrastructure is a boon to developers, since it
offloads a large operational burden for maintaining user data.  On the other
hand, developers find themselves having to leverage multiple services in a way
that both preserves their end-to-end storage requirements while
avoiding vendor lock-in.  This thesis presents and evaluates a new storage
architecture for overcoming this challenge in the context of two classes of
networked application: scientific computing workflows and decentralized
applications.

Scientific computing workflows and decentralized applications may
serve wildly different use-cases, but they operate under similar data-hosting
conditions.  First, they store and share data across multiple organizational
domains, such as different user groups, university networks, corporations, and
legal jurisdictions.  Second, each organizational domain has its own requirements
on how its users should store their data and make it available to others.  Third, data
management responsibilities in the application are distributed, where each
domain acts semi-autonomously.  There is no clear "application data owner" that unilateraly
decides hosting policies for all participants.

In practice, these applications are systems-of-systems, and stand to benefit
from leveraging existing systems instead of building new, bespoke ones from
scratch.  An application can make use of one or more \textbf{cloud storage}
providers to host its users' data,
and can distribute it to a scalable number of readers via one or more \textbf{content
distribution networks} (CDNs).  In addition, it can leverage data from
one or more external \textbf{curated data-sets} to provide better value to users.
For example, a navigation application like OpenStreetMap~\cite{openstreetmap} would host its users'
preferred routes, maps, and historic queries to the storage providers of their choice,
use a CDN to pre-fetch and cache map data to its appropriate geographic regions,
and use public weather data aggregated by NOAA to predict how long a commute may take.

The benefit to developers is obvious.  By relying on existing infrastructure,
they can reduce development time and focus primarily on their application's business
logic.  In a competative domain, this translates to reducing time-to-market,
and can make the difference between an application's success or failure.

However, there are two long-term risks with this strategy.  First, these
services are fundamentally unreliable.  They can unilaterally
change their pricing, feature-set, APIs, and availability.
Applications that rely on these services can break without warning,
and cost developers unforeseeable amounts of time and money.

Second, these services are heterogeneous.  Services that fill a similar role
do not offer uniform interfaces or well-defined semantics.
Without careful planning, the application will become tightly-coupled to the
services it uses by depending on it to behave a certain way (even if this
dependency is implicit).  This creates high infrastructure switching costs, making it
difficult for developers to address service unreliability or move to better
offerrings.

Designing applications to be portable mitigates both of
these risks.   However, the cost of porting $m$ applications to $n$ services
today requires $O(mn)$ patches.  This is true even if developers share their
patches---getting a patch to work with one application rarely requires zero
edits to work with another.  We do not believe this situation
will improve, since developers are compensated for shipping working
code rather than portable code.

Our solution is to port both applications and services to a shared
intermediate layer called a \emph{software-defined storage} (SDS) system.
This allows developers to ignore service-specific compatibility, and allows service
operators to \emph{port services to applications} by making them compatible with
the SDS system.  In doing so, we bring the portability cost down from
$O(mn)$ to $O(m + n)$ patches while preserving the application's end-to-end
storage requirements.

We achieve this with a shared layer of indirection.  Instead of focusing on
porting each application to each service, we focus on porting applications and
services to an intermediate layer.  We will have this intermediate layer mediate all
data interactions between applications and services.  The goal is that when a new service
is ported to this layer, all existing applications can make use of it without
modification.  We call a system that implements this layer a
\textit{wide-area software-defined storage} (SDS) system.

Wide-area SDS is unique in how it addresses application/service compatibility.
Unlike existing service compatibility frameworks, it focuses on preserving
the application's desired end-to-end storage semantics
irrespective of the underlying services' semantics.  Our prototype
system, called Syndicate, gives application end-points a coherent read/write
filesystem view of their data and offers developers a UNIX-y programming model
for describing the storage semantics their applications need.  In doing so, we
reduce the global number of portability patches from $O(mn)$ to $O(m + n)$.

\section{Contributions}

This thesis makes the following contributions:

\begin{itemize}

\item We present the design principles of software-defined storage, framed in
terms of prior work and the real-world storage needs of both scientific
workflows and decentralized applications.  We show how to
keep the number of portability patches limited to $O(m + n)$ while
both preserving end-to-end storage semantics and
respecting each organization's data-hosting policies
(Chatper~\ref{chap:design_principles}).

\item We present the design and implementation of Syndicate, a file-oriented SDS
system.  Syndicate is a real SDS system being deployed in scientific workflows today.
We show how Syndicate makes use of SDS design principles, and discuss lessons
learned in implementing SDS (Chapter~\ref{chap:syndicate_sds}).
We also show early performance numbers (Chapter~\ref{chap:evaluation}).

\item We show how to build SDS-powered applications.  We present the design and
implementation of three non-trivial applications on top of SDS: email, encrypted
file sharing, and automatic HPC data staging.  We show how each system can be
implemented on top of SDS such that each one reuses the same $n$
service-specific patches while requiring only a modest amount of
application-specific code for preserving storage semantics
(Chapter~\ref{chap:applications}).

\end{itemize}

