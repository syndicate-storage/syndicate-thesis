\chapter{Introduction}
\label{chap:introduction}

This thesis presents a practical way to 
implement user-centric storage
for distributed multi-user applications.
With user-centric storage, users host their
data on the servers of their choice and allow
the application to access it on their terms.
This stands in contrast to most
applications today, which follow a ``centralized'' storage principle
that places control over their data on application servers.

User-centric storage is a necessary data organizing principle in two major
application domains:  scientific computing, and peer-to-peer applications.
Science labs host and curate the data generated by local experiments, and share
it with remotely-executed scientific workflows subject to local data-specific curation
policies.  Peer-to-peer applications are designed to operate without central
points of failure, and as such, they host
user-owned data on the user's devices and replicate it to other peers subject to
the user's data-sharing policies.  In both cases, applications must allow
their users to control how their data is hosted and replicated in order to
function correctly.

Designing for user-centric storage has proven challenging to both users and
developers \emph{vis a vis} centralized storage.
Today's applications place extra data management
responsibilities on users, which they are unequipped to handle.
At the same time, developers must make their
code compatible with many different storage systems and preserve compatibility
in the face of user-initiated system upgrades.
To eliminate these burdens, we
propose a new user-centric storage design called \emph{wide-area
software-defined storage} (SDS).

The main contribution of this thesis is to demonstrate that wide-area SDS 
is a viable paradigm for building and deploying user-centric storage. 
We show that SDS preserves data ownership for users while offerring
the management convenience of centralized storage.
In addition, we show that SDS allows developers to implement and preserve
their desired storage properties without having to coordinate with users
whenever the storage systems change.

We have constructed a file-oriented SDS prototype called Syndicate,
which we have used to address
these in both scientific storage and peer-to-peer
applications.  Our evaluations show that Syndicate
preserves user ownership, safely automates
challenging data management tasks, and imposes only a modest
(constant) performance overhead.

\section{Data Ownership}

In this thesis, we are interested in reasoning about \emph{programmatic}
data ownership.  We say that a user
\emph{owns} a piece of data if she can unilaterally control how
other users' clients interact with it.  At a high level, this means that the
user be able must be able to do the following for a piece of data she owns:

\noindent{\bf Determine authoritative state.}  The user must be able to
identify original and authentic replicas to correct clients.  If the user cannot
do this, then clients have no way to tell whether or not the data they are
accessing was written by the user.

\noindent{\bf Mandate access methods.}  The user must be able to prescribe
the programmatic methods that a correct client must execute to read or write authoritative
replicas.  For example, a user could require a client to decrypt the
received data with a particular key when completing the read request.

\noindent{\bf Advise consistency.}  Given two or more authoritative but divergent replicas,
the user must be able to advise correct clients on how to determine consistent
state from them.

We believe that this is a reasonable high-level definition of ownership,
since if user delegates any of these responsibilities to
another principal, then that principal would
have the power to direct application clients' reads and writes to data that the
user did not create (we present a more rigorous definition in
chapter~\ref{chap:design_principles}).
We call the logic that enforces ownership the \emph{control-plane logic} for the
user's data.

The control-plane logic is a program that decides how every request on the user's
data replicas will be handled.  However, it is not visible to users
in centralized applications,
since centralized applications implement it on their behalf
(Figure~\ref{fig:intro-centralized-overview}).  Users instead
advise the application on how to evaluate the logic.
For example, a social media
application like Facebook lets a user decide who can see which of her photo albums.  As
another example, a collaborative document-sharing application like Google Docs
lets a document owner decide which changes go into the final version of the document.

\begin{figure}[ht!]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/intro-centralized-overview}
   \caption{\it High-level design of centralized application storage.
   The application owner executes the control-plane logic with the advice of the
   users that store their data in its storage servers.  As a result, the
   application operators are the de-facto data owners.}
   \label{fig:intro-centralized-overview}
\end{figure}

When the application implements and executes the control-plane logic, the application's
operators are the programmatic owners of all user data.  By contrast, if the user
implements and executes the control-plane logic for her data, then the user
is the programmatic data owner.

\section{User-centric Storage Today}

Ensuring that the user implements and executes their data's control-plane
logic requires hosting it on computers she controls.  This is achieved today
using a design strategy we call the ``bring-your-own-storage'' approach.

With bring-your-own-storage, a user has
the application client write her data on her servers, and has the application
servers route read-requests from other users to her servers directly
(Figure~\ref{fig:intro-user-centric-overview}).  This way, the user mediates all
data interactions.


\begin{figure}[ht!]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/intro-user-centric-overview}
   \caption{\it High-level design of a user-centric application storage.
   Data owners directly control the control-plane logic by means of controlling
   the data servers.  This grants them de-facto data ownership.}
   \label{fig:intro-user-centric-overview}
\end{figure}

This is the user-centric storage strategy employed by both peer-to-peer applications and 
scientific computing applications.  Peer-to-peer applications store a user's
data directly on their devices, making the devices responsible for evaluating
the user's control-plane logic.  Scientific applications store the lab's
data to their on-site lab servers so it can be shared with applications running
on behalf of other labs.

While bring-your-own-storage works today, it has several major shortcomings that
make applications hard to develop, deploy, and manage.  We seek to remove them
with software-defined storage.

\subsection{Peer-to-Peer Storage}

Most prior work on peer-to-peer applications
focuses on implementing one of two variants of bring-your-own-storage.
Either each user stores and serves their
data out of their personal computers, or a set of mutually-trusting
users cooperate to host each other's replicas.
% TODO: cite some systems that follow both paradigms

Neither approach works well in practice, since they require users to carry
out data management tasks for which they are largely unqualified.
In particular, \textbf{users must keep their devices online to service requests}.
This means that users must regularly maintain them and audit them
for problems.

Unfortunately, maintaining always-on devices is something that
users consistently fail to do, leading to broken software and comporomised hosts with arbitrarily
bad consequences for users.  Consumer devices are not self-managing,
and remotely-exploitable holes are regularly discovered and leveraged
for criminal activity~\cite{iot-weaknesses}~\cite{mining-your-ps-and-qs}~\cite{router-exploits}.
In many cases, vendors actively prevent users from fixing
them~\cite{eff-reports} or even disclosing the problems~\cite{sueing-researchers}.
This is unlikely to improve anytime soon, since most users today can barely
interact with their computers correctly~\cite{un-study}, much less secure them.

Even if all users kept their always-on devices in good working order,
serving data out of a personal device is an inadequate for handling a scalable
number of reads and writes.  In these cases, today's peer-to-peer
applications \emph{cooperatively} service reads, in order to overcome the
low upload bandwidth available at the ``edge'' of the Internet.

Cooperation makes preserving ownership problematic, since users must continue
to ensure readers receive authoritative data even though other users are
servicing requests.  While digital signature algorithms are often proposed to prove
data authenticity, they are problematic in practice because they
\textbf{require users to share, secure, and revoke key pairs out-of-band}.
Many studies~\cite{why-johnny-cant-encrypt}~\cite{why-johnny-still-cant-encrypt}~\cite{why-johnny-still-still-cant-encrypt}
have shown that users consistently fail to do this correctly, even when failure
causes them to lose large amounts of money~\cite{mt-gox}~\cite{bitfinex}.

% cite: why johny (still (still)) can't encrypt,
% cite: that recent (UN?) study about computer literacy being super-rare
% cite: any other usability studies we can find for managing servers (maybe the
% inadequacy of IoT device security?)
% cite: all Bitcoin hacks (bitcoin relies on keeping your private key(s) secure)

% Mike Freedman believes that most p2p apps work simply
% because enough people are altruistic.  It's unclear that
% incentive mechanisms have all that great of an impact,
% especially in small-scale deployments (e.g. a bootstrapping system).

\subsection{Scientific Data Storage}

From a developer's perspective, a major problem with users running their own
storage servers is that they must agree on operational semantics.  For example, if Alice shares data
with Bob, then Bob must know what consistency guarantees Alice server makes when
he reads or writes to it.  If Alice offers different guarantees than what Bob
expects, then it's up to Bob's client to make up the difference.  In other
words, Bob needs a ``driver'' for Alice's storage.

% agree on consistency
% advise consistency? pki problem
% lifecycle? must upgrade at owner's discression
% access control? either trust centralized service, or owner can force upgrade
% replica selection? must upgrade at owner's discression

This problem arises in scientific computing, where each lab has its own storage
system and often their own consistency models.  Scientists must account for
storage heterogeneity in the design of their applications, and often employ
client-side libraries with drivers to help do so (such as Parrotfs~\cite{parrotfs},
libcloud~\cite{libcloud}, and iRODS microservices~\cite{irods}).

In most contexts, having a separate library of drivers would be
good engineering practice because it reduces code duplication and decouples
application logic from storage implementations.  In user-centric storage,
however, this creates a liability since \textbf{upgrading storage drivers and
storage systems must be atomic with respect to data interactions}.  If this is
not the case, then obsolete clients may continue to read and write invalid data,
leading to incorrect behavior.  In the case of scientific computing, this can
mean costly experimental failure.

Ensuring upgrades are atomic requires human-in-the-loop coordination,
since it must involve all parties who
use the storage drivers.  For example, the data owner (i.e. the lab)
will need to orchestrate a ``flag day'' at which point the storage system will
be upgraded and everyone must remember to stop all ongoing experiments and
switch over to the new drivers.

Another wrinkle with distributing storage drivers is that it
forces downstream users to identify trusted software
repositories that can serve the latest software.  Often times the data owner
itself can fulfill this role, but this nevertheless reduces to labs and their
scientists having to manually manage key pairs in practice.

\subsection{Discussion}

Obviously, none of these problems exist for centralized
application storage.  Since users trust the application to handle all aspects
of data ownership, the application can place all authoritative replicas
and control-plane logic onto professionally-managed, highly-available
servers connected to the Internet via multi-homed high-bandwidth connections.

With centralized storage, users do not need to keep any devices online to serve their data, they
do not need to keep any software up-to-date, and they do not need to manage any trust
relationships beyond that with the application.  This is much more convenient
for them, since it reduces interacting with data to simply loading a Web page.
This is also more convenient for application developers, since they control all
aspects of the application software lifecycle and do not have to accomodate
third-party components.

An open question is to what extent these shortcomings are artifacts of the
bring-your-own-storage apporach to user-centric storage, as opposed to
fundamental limitations of user-centric storage itself.  In this thesis, we
show that all of these shortcomings can be overcome with an alternative
storage design strategy that separates responsibility for executing
control-plane logic from maintaining a server to run it.

\section{Separating Servers from Ownership}

Implementing user-centric storage with the operational convenience of centralized storage
requires re-thinking the relationship between the control-plane logic, the servers
that execute it, and the servers that host data.  To do so, we make three critical observations.

First, we note that the control-plane logic is
orthogonal to the ``data-plane logic'' that loads and stores raw bytes over the
network.  If the data owners' computers had some way to
delegate the data-plane logic to untrusted computers, then they could leverage
the same highly-available and professionally-maintained storage
infrastructure that centralized applications use for hosting replicas.
In other words, we note that we can split all communications between data owners
and clients into two logical communication channels: a
\emph{data-plane channel} for fetching and storing
the raw bytes, and a \emph{control-plane channel} 
for handling the request and translating the replicas'
raw bytes into an authoritative, consistent view of the data.
(Figure~\ref{fig:intro-two-channels}).


\begin{figure}[ht!]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/intro-two-channels}
   \caption{\it Logical separation of data-plane from control-plane.
   The region above the dashed line corresponds to the data-plane, where users
   operate on raw data bytes (blue speech bubbles) via commodity storage
   infrastructure (blue disk).  The region below
   corresponds to the control-plane, where the data owner specifies to other
   users how to interpret the raw bytes into a consistent, authoritative view of
   the data (yellow speech bubbles).}
   \label{fig:intro-two-channels}
\end{figure}

Second, we observe that while the control-plane logic may have an
an arbitrary implementation, in practice it is usually expressed in terms
of how to carry out a specific operation on a specific datum by a
specific user.  We can partially-evaluate the control-plane logic into 
a set of \emph{interaction functions} for each $(user, datum, operation)$
triple that exist in a larger \emph{execution context}.
The $operation$ variable captures the type of interaction (e.g.
``read'', ``write''), and the $user$ and $datum$ variables uniquely identify the
principal carrying out the operation and the set of replicas to operate on.

The interaction function is a bound variable in a larger execution context,
which binds values to all other variables in the interaction function.
When run within its correct execution context,
the interaction function carries out its operation (including side-effects) on
the its datum on behalf of its user.

To a first approximation, an
interaction function can be thought of as a control-plane wrapper for a
storage driver.  The driver itself operates on the data plane where it
loads and stores a datum's bytes from the storage
system.  The interaction function invokes the storage driver by
pre-processing the request and post-processing the driver's output to
determine how the $datum$ bytes ought to be handled for the given $operation$ and $user$
(Figure~\ref{fig:intro-interaction-function-overview}).

\begin{figure}[ht!]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/intro-interaction-function-overview}
   \caption{\it Relationship between interaction functions, their execution
   contexts, and storage drivers.
   The driver implements the data-plane logic for loading and storing data in
   Alice's storage (step 2), and the interaction function preserves Alice's ownership of
   \textit{photo.jpg} by mediating all requests to it (steps 1 and 3).}
   \label{fig:intro-interaction-function-overview}
\end{figure}

Neither centralized nor user-centric storage systems today implement
interaction functions and execution contexts as first-class
logical entities, but it is easy to see how this might work.  Consider a
centralized Dropbox-like application that offers users a ``read'' and a ``write'' operation
on their data.  If Alice shares a
file with Bob but not Charlie, then
the application effectively implements six interaction functions that handle
read and write requests from Alice, Bob, or Charlie.  Charlie's functions simply
NACK his reads and writes; Bob's read function runs the storage driver code to serve him Alice's data
while NACKing his writes; Alice's functions serve her data on read and allows
her write requests to change it.  The execution contexts for
Alice and Bob include ancillary data like the credentials Dropbox
requires for reading and writing the file.

In user-centric applications, each data owner implicitly runs the set of interaction
functions for her data on their servers.  For example, in a peer-to-peer messaging application
Alice's computer allows Bob to ``read'' her messages to him.
Alice may both ``read'' and ``append'' to the list of messages to Bob.
Bob offers a complementary interface to Alice, thereby allowing them to
communicate.  Their interaction functions enforce message ordering internally, ensuring that
both see the conversation in the same order.  Extra information such as Alice's and
Bob's public keys and message sequence numbers are supplied in their interaction
functions' execution contexts.

% TODO: table 

When we think about control-plane logic as a set of interaction
functions running in execution contexts, we thirdly observe
that each one will only be executed \emph{at the moment the user interacts
with the datum}.  If we could design them such that the \emph{only feasible way} for
the $user$ to carry out the $operation$ on the $datum$ is to execute the
\emph{owner-given} function in an \emph{owner-given} execution context, then we can delegate 
control-plane execution from the owner's computers the client's computer.
Careful design would allow the owner offload the control-plane execution to users.

\subsection{Control-Plane Offloading}

The key to separating ownership from servers is to transfer control-plane
execution to users' computers, thereby implementing a distributed control-plane logic
where data owners do not need to process each request.
The challenge is to so in a way that is both efficient
and preserves data ownership.

A naive but illustrative way to do this
is to have the data owner generate and sign an interaction function and
execution context for each
$(user, datum, operation)$ triple (Figure~\ref{fig:intro-sds-overview}).
The owner would then encrypt the two of them
with its user's public key and replicate the ciphertext to commodity storage.
They would be designed to encrypt the data owner's writes with the user's public
key.

\begin{figure}[ht!]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/intro-sds-overview}
   \caption{\it Overview of control-plane offloading.  User 1's interaction
function allows his requests and serves him fresh, authoritative data from
commodity cloud storage.  User 2 does not have an interaction function, so he
has no way of accessing the owner's data.}
   \label{fig:intro-sds-overview}
\end{figure}

Only the user's client would be able to fetch, decrypt, and execute the
designated function, and the user could verify that the function came from the
owner.  Combined with data encryption, this allows the owner to enforce data 
access control since only the allowed users would have decryptable interaction functions.

The owner would design the interaction functions to preserve authenticity
and consistency.  The function would only operate owner-signed data, but any
data the user generates with it would be signed by the user.  The function would
preserve the desired consistency by controlling the order of its execution
relative to other functions.

To see how this might work in our file-sharing example, Alice could leverage
an existing Dropbox-like application to implement the data-plane for a separate
user-centric file-sharing application.  To
share a file with Bob, Alice encrypts the file with his
public key, signs it with her private key, and uploads the ciphertext to Dropbox. 
She does likewise with her interaction function for Bob.

Bob's client fetches and decrypts his interaction function and execution context,
and runs it in order to fetch, verify, and decrypt the associated file.
Alice includes a monotonically-increasing timestamp in the file header each time she
writes to it, which Bob's interaction function inspects to ensure that he
never receives a stale replica (e.g. it blocks Bob's reads until it downloads
file data with a fresh timestamp).

Neither Dropbox nor Charlie would be able to obtain a
copy of Alice's file plaintext or forge Alice's signature, so neither one could
convince Alice or Bob that they have authoritative or fresh copies.  
Dropbox could equivocate to Bob about which ciphertext it has
by masking Alice's future writes, but Alice could mitigate this
by programming his interaction function to obtain her consistency metadata via a separate
channel.

\section{Wide-area Software-defined Storage}

This file-sharing example gives a high-level sketch of how 
to leverage untrusted commodity storage providers as a data-plane
and leverage other users' computers to run the control-plane.
It highlights the importance of having the data owner control which
interaction functions and execution contexts each user may run.

Controlling the distributed control-plane requires addressing several non-trivial 
challenges.  First, we need a design methodology for interaction functions and
execution contexts.  Once we can formulate them, we need a way to securely
deploy them to their users in a way that does \emph{not} force users to become experts in
public-key cryptography.  This requires secure automation.
Since the control-plane is distributed across an untrusted network, we need a
way for the data owner to safely update it in the presence of network partitions and adversarial
behavior.  Finally, we need to do all of this in a way that scales in the amount
of data, the amount of aggregate bandwidth, and the number of users.  Our
solution is \emph{wide-area software-defined storage} (SDS).

% TODO: figure of EC

The ultimate goal of wide-area SDS is to leverage untrusted commodity infrastructure to
give users their own scalable personal storage volumes without requiring them
to become experts at managing storage servers.  Each user would create a storage
volume for each user-centric application account, and have the application
client load and store account data via the volume.  We show that this not only preserves data
ownership for users, but also simplifies both centralized and user-centric application design.

The remainder of this thesis is organized as follows.
Chapter~\ref{chap:design_principles} describes the design principles of
software-defined storage, framed in terms of prior work and the real-world
storage needs of peer-to-peer and scientific applications.  In particular, we show
how SDS must organize data in order to correctly implement distributed
control-plane logic.

In Chapter~\ref{chap:syndicate_sds}, we present a prototype SDS system called
Syndicate, which organizes user data into a filesystem abstraction.  In
Chapter~\ref{chap:applications}, we describe three real-world applications built
with Syndicate in the realms of both scientific data storage and peer-to-peer
applications, and show that SDS only requires a modest amount of new code to
make applications compatible with new storage systems.  In
Chapter~\ref{chap:evaluation}, we show that Syndicate imposes only modest
performance overhead.  Syndicate-powered applications take the performance
characteristics of the storage systems their users choose, as desired.  We
conclude in Chapter~\ref{chap:conclusion}.

