\chapter{Introduction}
\label{chap:introduction}

The proliferation of cloud-based services poses new opportunities and
challenges for hosting data.  On the one hand, the availability of
professionally-maintained infrastructure is a boon to developers, since it
offloads a large operational burden for maintaining user data.  On the other
hand, developers find themselves having to leverage multiple services in a way
that both preserves their end-to-end storage requirements while
avoiding vendor lock-in.  This thesis presents and evaluates a new storage
architecture for overcoming this challenge in the context of two classes of
networked application: scientific computing and conventional Web
applications.

Scientific computing workflows and Web applications may
serve different use-cases, but they operate under similar data-hosting
conditions.  We identify three commonalities:

\begin{itemize}

\item They store and share data across multiple \emph{organizations},
each of which constitute a group of one or more users working towards
a single purpose.  Organizations include 
user groups, university networks, corporations, and
legal jurisdictions.  Organizations may overlap, and a user may belong to
multiple organizations.

\item Each organization has its own requirements and policies
on how its users should store their data and make it available to others.

\item Organizations must be able to influence how other organizations interact
with its users' data.

\end{itemize}

\section{The Systems-of-Systems Approach to Storage}

In practice, these applications are \emph{systems-of-systems}.  Their code
executes across multiple organizations, and is realized by aggregating
functionality across multiple unrelated systems spanning multiple organizational
domains.  For example, campus webmail is a system-of-systems, since it combines DNS
servers, SMTP servers, and a federated identity system to grant students and
faculty access to their email via their Web browser.

Systems-of-systems developers stand to benefit
from leveraging existing systems instead of building new, bespoke ones from
scratch.  An application can make use of one or more \textbf{cloud storage}
providers to host its users' data,
and can distribute it to a scalable number of readers via one or more \textbf{content
distribution networks} (CDNs).  In addition, it can leverage data from
one or more external \textbf{curated data-sets} to provide better value to users.
For example, a navigation application like OpenStreetMap~\cite{openstreetmap} would host its users'
preferred routes, maps, and historic queries to the storage providers of their choice,
use a CDN to pre-fetch and cache map data to its appropriate geographic regions,
and use public weather data aggregated by NOAA to predict how long a commute may take.

One significant consequence of building systems-of-systems applications
is that a non-trivial amount of design and implementation efforts go towards
preserving end-to-end data hosting and access semantics.  For example, Web
application servers must coordinate with downstream CDN nodes to ensure that
clients read fresh data.  As another example, scientific compute clusters at
different labs must establish trust in a shared single-sign-on service (like
InCommon~\cite{incommon}) to allow scientists in one lab to access data in
another lab.

Despite this extra effort, the benefits offerred by the system-of-systems
approach to application design are obvious.  By relying on existing infrastructure,
developers can reduce development time and focus primarily on their application's business
logic.  In addition, developers can amortize the operational cost of
maintaining infrastructure across many different applications by using these
shared resources.  This translates to reducing the time it takes to build a
working system, leading to faster product iteration and faster turn-around time
for experiments.

\section{The Cost of Portability}

However, there are two long-term risks with the system-of-systems approach.
First, these underlying services are unreliable.  They can unilaterally
change their pricing, feature-set, APIs, and availability.
Applications that rely on these services can break without warning,
and cost developers unforeseeable amounts of time and money.

This exact relationship between service clients and the service operators is often encoded
in the operator's terms of service.  Existing terms for popular services
explicitly state that the operators have the ability to affect unilateral
changes~\cite{amazon-tos}~\cite{google-tos}~\cite{dropbox-tos}.
% example: Dropbox API v1 to v2 sunsetting, Google deciding to retire services

Second, these services are heterogeneous.  Services that fill similar roles
do not offer compatible interfaces or semantics.
Without careful planning, the application will become tightly-coupled to the
services it uses by depending on it to behave a certain way (even if this
dependency is implicit).  This creates high service switching costs, making it
difficult for developers to address service unreliability or move to better
offerrings.

Designing applications to be portable mitigates both of
these risks.   However, the cost of porting $m$ applications to $n$ services
today requires $O(mn)$ patches.  This is true even if developers share their
patches---getting a patch to work with one application can require completely
re-writing it to work with another application.

We do not believe this situation will improve,
since in both domains developers are incentivized to ship code that \emph{works
today} as opposed to code that is portable to unspecified systems at unspecified
times in the future.
Even if portability was a desireable and achievable design goal from the get-go,
getting $m$ applications to adopt a new service's behavior would still at best require $O(m)$
work.

\section{Software-defined Storage}

Our solution is to port both applications and services to a shared
intermediate layer.  Instead of focusing on
porting each application to each service, we focus on porting applications and
services to an intermediate layer.  We will have this intermediate layer mediate all
data interactions between applications and services.  The goal is that when a new service
is ported to this layer, all existing applications can make use of it without
modification.  We call a system that implements this layer a
\textit{wide-area software-defined storage} (SDS) system.

SDS offers two key benefits over the stats quo.  First, it brings the portability cost down from
$O(mn)$ to $O(m + n)$ patches while preserving the application's end-to-end
storage requirements.  Second, it removes the need for service operators and
application developers to coordinate service changes.  A service operator can
unilaterally change the behavior of her service by modifying the service and
shipping a patch to fix all applications, and a developer can unilaterally
change from one service to another without having to patch the application.

We have built two SDS prototypes to validate the effectiveness of our approach,
as well as several sample applications.  One of our prototype
systems, called Syndicate, gives scientific application end-points a coherent read/write
filesystem view of their data and offers developers a UNIX-y programming model
for describing the storage semantics their applications need.  Our other
prototype system, called Gaia, gives Web aplication endpoints a key/value store
that allows each user to host their own portion of the application state.
In both cases, we reduce the number of portability patches from $O(mn)$ to $O(m + n)$
for supporting additional existing storage services and for adding new semantic
features.

\section{Contributions}

This thesis makes the following contributions:

\begin{itemize}

\item We present the design principles of software-defined storage, framed in
terms of prior work and the real-world storage needs of both scientific
workflows and Web applications.  We show how to
keep the number of portability patches limited to $O(m + n)$ while
both preserving end-to-end storage semantics and
respecting each organization's data-hosting policies in a coordination-free
fashion. (Chatper~\ref{chap:design_principles}).

\item We present the design and implementation of two SDS systems: Gaia and
Syndicate.  Syndicate is a real SDS system being deployed in scientific
workflows today, and Gaia is a real SDS system being deployed to build
``serverless'' Web applications (i.e. Web applications that can operate
without the need for application-specific servers).
We show how Gaia and Syndicate make use of SDS design principles
(Chapter~\ref{chap:syndicate_sds}).

\item We show how to build SDS-powered applications.  We present the design and
implementation of non-trivial SDS-powered applications \emph{that could not
have been feasibly built without SDS}.  Among these are an end-to-end encrypted
Webmail client that removes the user from key management, a server-less
groupware application that lets users control how their data gets hosted and
accessed, and a scientific data-staging application that
automatically makes fresh datasets available from existing data repositories to
HPC clusters via commodity CDNs.  In all cases, we reuse the $n$
service-specific patches across all applications
(Chapter~\ref{chap:applications}).

\item We present early performance numbers for Gaia and Syndicate, both in the
form of microbenchmarks and in real-world performance of applications built on
top of them (Chapter~\ref{chap:evaluation}).

\end{itemize}

