\chapter{Software-defined Storage Systems: Design and Implementation}
\label{chap:syndicate_sds}

In order to vet our design principles for software-defined storage, we used them
to design and implement two production-quality systems.  The first system,
called Gaia, implements a global key/value store with programmable semantics for its
\texttt{get} and \texttt{put} operations.  It is a minimalistic SDS system---it
provides just enough functionality to ensure that applications and
(non-technical) users can interact with their under a fixed set of 
semantics without relying on any fixed third-party services.

The second system, called Syndicate, implements
a full POSIX filesystem interface with programmable semantics for most
filesystem operations.  It is much more featureful than Gaia, and is designed
to port existing scientific computing workloads to commodity third-party
services.

The overall approach to the design of these two systems begins with identifying
organizational boundaries.  This lets us characterize the access and mutate
flows that the volumes will need to support in terms of how they organize data,
and in terms of what API contracts must be offerred by the service and
aggregation driver models.  Once we know these things, we can proceed to design the
gateways and metadata services.

\section{Gaia: a Global Key/Value SDS System}

Gaia is a global key/value store designed for allowing users to host their data
for decentralized applications.  We say ``decentralized applications'' to mean
applications where all of the business logic computations occur on the
users' computers.  For example, a decentralized todo-list
application~\cite{blockstack-todo} would fetch the user's application state from
the storage providers of their choice, allow the user to interact with the items
once loaded, and would store the resulting state back to the storage providers
when the user is done.  Unlike competing applications like Google
Calendar~\cite{google-calendar}, there is no ``application server'' that
runs business logic computations over the user's data.

Gaia is designed primarily for Web programming environments, and offers two
modes of operation.  The first mode, called ``single-player mode,'' offers
behavior similar to what Web developers today expect from HTML5
\texttt{localStorage}~\cite{w3c-localstorage}.  The Web code can load a value
given a key and store a (key, value) pair, with the expectation that only this
instance of the code will be able to interact with the key.  This is the mode
used by our example todo-list application---a user only interacts with their own
todo-list data, and users cannot read or write to each other's data.

The second mode, called ``multi-player mode,'' offers one-writer many-readers
semantics.  Only a user may write to their own keys, but any user may discover
and read their keys.  For example, a decentralized blogging application would
use Gaia's multi-player mode to allow a user to publish blog posts, and allow
other users to read them.

The main contribution of Gaia is that it gives Web developers a secure and
reliable way to outsource data-hosting to users.  Gaia ensures that each user
securely discovers each other users' \emph{current public keys} and \emph{data
source URLs} for each application they use prior to loading the data.
In doing so, Gaia offers end-to-end
data authenticity and confidentiality while using untrusted commodity cloud infrastructure
to host and serve application data.

We present Gaia as a proof-of-concept system for demonstrating SDS design
principles.  It is a production system today and is used by
Blocksatck~\cite{applications} as the primary way for hosting user data.

\subsection{Motivation}

In designing Gaia, we set out to ensure that users ``own'' the data that they
write.  By this, we mean that a user unilaterally decides where authoritative
copies of their data are hosted, and how can access them.  In doing so, we 
(1) allow users to keep their data in the event that the application disappears, we
(2) allow users to shop around for different competing applications by granting them
access to their data, and we (3) allow developers to avoid the operational burden of
hosting any state on their users' behalf.

In many cases, a Web application's data interaction model is centered around
individual user activity.  Users can read and write their own data, but the can
at best read other users' data.  Users rarely have the ability to write to the
same data, and each datum belongs to at most one user.

For example, this is
true of the data interaction models for most social media applications,
most photo-sharing applications, and most blogging applications.  In systems
that present ``shared-write'' views of data, like a comment section on a blog or
a shared Google Document page~\cite{google-docs}, the data interaction model
nevertheless attributes each write to a specific user (and then ``merges''
writes to present a consistent view).  In all these cases, each write is
attributed to exactly one user.

Even though a Web application has a single logical database that holds all of
its users state, this observations allows us to reformulate
the global database as a collection of single-writer multi-reader user-specific
databases.  It becomes the application client's job to translate a set of reads across
the users' databases into a consistent view (whereas this had traditionally been
done by the application's global database).  When put into SDS terminology, we
say that \emph{each user is its own organization, and only the organization may write
to its own data} (but others may read from it).

This reformulation of application storage gives us the ability to decouple each
users' database from the administrative domain in which it runs.  The
application only needs to be able to read from a user's database in order to
present other users with a view of its data.  The database need not reside on
servers that the application developer chooses.  What this means in SDS terms is
that there is one volume per (application,user) pair, and that only the user may
write to the volume and control its access semantics.  Application developers
are simply considered users of their own application.

A user may optionally make a volume readable to another set of users, or to the
world.  For example, users of a blogging application would make their volumes
world-readable.  As another example, the application developer's volume would
store application assets like images, CSS, and code to be loaded at runtime by
the running application code (thereby allowing the developer to push
updates to their application, much like how they do today on the Web).

Our SDS design principles come into play in the following tasks:
\begin{itemize}
   \item \textbf{Multiple Storage Systems}.  We will show how Gaia allows users
      to choose the storage systems that will host their data in an
      application-agnostic way.
   \item \textbf{User Storage Policies}.  We will show how Gaia allows users to
      stipulate programmatic policies pertaining to data availability and durability.
   \item \textbf{Application-specific Views}.  We will use SDS aggregation
      drivers to show how to construct a
      global, consistent view of a set of users' databases.
\end{itemize}

\subsection{Blocks, Manifests, and Volumes}

Gaia organizes a user's data into a set of volumes, where each volume
holds one application's data.  The user decides whether or not the volume
operates in single-player or multi-player mode, and selects the set of
service drivers to use to replicate data.

A volume in Gaia is a key/value store.  The API the Gaia client exposes to
programmers resembles HTML5's \texttt{localStorage}.  It offers three methods:
\texttt{get(key) -> value}, \texttt{put(key, value) -> bool}, and
\texttt{delete(key) -> bool}.

Internally, the set of keys in a volume are bundled into a single manifest.
Each value is the associated block.  This means that writes are serialized
across keys, and that writes to a key are atomic.  These behaviors were chosen
specifically to emulate the \texttt{localStorage} API contract.

The service drivers for a volume must implement at least per-key sequential
consistency.

\subsection{Gateways}

% TODO: figure of Gaia

Whenever the application client writes new data, the new block and manifest are
generated within the Web browser and sent to a co-located Gaia node
(Figure~\fig{fig:gaia-overview}).  The Gaia node implements an SDS gateway for
each application volume.  Upon receipt of the new data, it loads and
synchronously invokes the volume-specific service drivers to replicate the key
and value to the users' back-end storage providers.  When the application later
reads the key, the user's Web browser contacts the Gaia node to (1) load up its
volume, and (2) load and return the value for the requested key.

Users run a Gaia node for each of their devices.
In Gaia, we make the consistency assumption that \textbf{at most one device will
be writing to a given volume at any given time}.  This is a reasonable
assumption in practice, because (1) a volume may only be written to by the user
that owns it, and (2) a user typically does not access the same application from
two different devices simultaneously.

We use this assumption to side-step the need for a user's Gaia nodes to
coordinate to resolve write-conflicts and coordinator-changes.  Since writes are
always serialized through the user, they will not conflict.  The writer Gaia
node is always the coordinator, and the current coordinator for a volume in Gaia
is simply the last writer.

\subsection{Metadata Service}

A user's Gaia nodes
implement a peer-to-peer metadata service.  The Gaia MS is based on prior work
on Blockstack~\cite{blockstack}~\cite{virtualchain}~\cite{ali2017}.  It enables
Gaia nodes to both ensure that readers do not see stale data, and to ensure that
any Gaia node can discover and read keys from a given volume for any user.

Gaia uses a blockchain-based SSI system both for bootstrapping trust between
users and for implementing the ``volume discovery'' functions of its MS.  When the
user registers a name in the SSI, she includes a cryptographic hash within the
blockchain record.  This hash corresponds to a ``zone file'' that
contains URLs to the user's list of Gaia volumes.

Gaia nodes work with the SSI system to find both find the set of names and public keys
as well as the set of zone file hashes.  They self-organize into an unstructured
peer-to-peer network called the Atlas network~\cite{blockstack-white-paper}
through which they exchange zone files.  They exchange
bit-vectors with one another to announce the availability of their zone files,
and exchange zone files with one another in rarest-first order such that all
Gaia nodes eventually have a 100\% replica of all zone files.

Since Gaia nodes view the same blockchain, they calculate the same sequence of zone
file hashes.  This gives them a ``zone file whitelist'' that grows at a constant
rate (no faster than the blockchain).  They use the whitelist to identify only
legitimate zone files, and rely on the blockchain to ensure that not too many
new zone files can be introduced into the system at once.  A detailed
description of the peer network can be found in~\cite{ali2017}.

% TODO: figure of Gaia volume lookups

The Atlas network ensures that each Gaia node both knows the current public key
and current zone file for each user.  Each user's
zone file points to a set of signed JSON web tokens (JWTs).  Each JWT contains the
public keys for each of the user's devices, the public keys and URLs to
replicas of each of the user's volume descriptions.  This way, a Gaia node can
look up an application-specific volume for a user given the user's name on the
SSI system~\ref{fig:gaia-volume-lookups}.  Importantly, the networks and storage
providers hosting zone files, JWTs, and volume descriptions are \emph{not} part
of the trusted computing base.

\subsection{Aggregation Drivers}

Users specify end-to-end storage semantics in Gaia by standing up and running
publicly-routable Gaia nodes to process their writes, and handle reads from
other users.  To do so, the user registers a name for the Gaia node in the SSI
system and records an IP address in its associated zone file.
Then, when the user creates a volume, she simply lists the Gaia node's SSI name
in the volume description as the "read" endpoint.  When other users go to
read from her volume, their Gaia nodes issue the request to the user's Gaia node
indicated in the volume description.  This allows the user to announce
instructions on how to access her data.

A Gaia node's zone file contains a link to a signed
JWT that encodes its per-volume processing instructions.
This includes code for the Gaia node to use to process other
users' reads, the code to process the node owner's writes, an optional
per-volume ``next hop'' hint that identifies the next Gaia node to which the data flow will be
forwarded, and the cryptographic hash of the
current JWT.  The presence of the cryptographic hash 
allows the node owner to announce node configuration changes
by sending new blockchain transactions, which imposes an auditable,
totally-ordered log of configuration changes (i.e. the user does \emph{not} need to
rely on the host that serves the volume JWT to serve fresh data).

% TODO: read and write diagram

When handling an access flow, the user's Gaia node first inspects the volume
record to determine the next-hop.  It
forwards the request to this Gaia node (looking up its IP address in the SSI
system), which in turn may either service the request for data, or forward it
along as well.  This resolution process continues recursively, until a Gaia node
loads data from a storage provider.  When it returns the chunks, the Gaia nodes
along the request path execute their application-specific stages to process it
en route back to the reader (Figure~\ref{fig:gaia-reads-and-writes}).

Writes work in a similar fashion.  The user's volume record identifies the
``write'' Gaia node to which to forward new chunks.  Upon receipt of chunks to
store, the Gaia node executes its mutate flow stage logic and forwards the
resulting chunks on to a ``next-hop'' Gaia node.  Eventually, the chunks reach a Gaia
node that will replicate them to underlying storage systems.

The code for each stage is identified by its cryptographic hash in the node's
volume description.  This allows the application to inspect the path of Gaia nodes that
will process access and mutation flows, and determine that the set of nodes are
correctly configured before executing the read or write.

Each Gaia node operator can reprogram the node's behavior by updating the zone file.
This ensures that all other nodes will see the change to the node's behavior (or
at least see that the code may have changed, if only the zone file hash is
discovered by the SSI system's blockchain indexing).  Upon noticing that their
zone file has changed, the Gaia node fetches and installs the new code from a
well-known URL resource record listed in the zone file contents.

\section{Administration}

To minimize coordination between developers and users, we
automate as much of the system administration as possible.
In its day-to-day operation, the only administrative contact a user has with their volumes is
in connecting storage providers, which is handled via a provider-specific Web UI
that automates creating and sharing an OAuth2~\cite{oauth2} token.  In addition,
we minimize the instances where the user directly interacts with cryptographic keys
by ensuring that they only need to do so when they acquire or lose a personal
computing device.

Application developers do not interface directly with storage providers, but
instead with the user's designiated Gaia node (usually a locally-hosted daemon,
but optionally a cloud-hosted daemon if the device, such as a smartphone, cannot
run daemons).  Instead, developers specify the storage requirements the
application needs, and the Gaia node pairs the requirements with storage drivers
when creating its volume.  A table of storage requirements can be found in
Table~\ref{tab:gaia-storage-requirements}.

% TODO: table of Gaia storage classes

The application code discovers a user's Gaia node as part of the SSI sign-in
process.  The SSI service identifies to the application the network address
of the user's Gaia node.  The application then learns the set of Gaia storage
providers, and the set of capabilities they offer (which can be matched to
storage requirements).

The resulting storage administration workflow for users and developers works as
follows:
\begin{itemize}
   \item When the user creates an account in the SSI service, she connects one
      or more storage providers to her account.
   \item The user loads the application and clicks its "sign-in" UI element.
   \item The application redirects the user to the SSI service's "sign-in" UI,
      which prompts the user to authorize the sign-in request.  Specifically,
      the user is presented with the application's request for either a
      ``single-player'' or ``multi-player'' volume.
   \item Once approved, the SSI service redirects the user back to the
      application, passing it a session token which identifies the user's Gaia
      node.
   \item The application requests a volume.  If this is the first such request,
      the Gaia node creates an application-specific volume.  The node then
      returns a handle to the volume which the application subsequently uses to
      load, store, and delete keys.
\end{itemize}

At no point are users asked to interact with volume, user, or gateway keys, and at no point
are users asked to perform access controls.  At no point are the developers
asked to identify or bootstrap a connection to storage providers, and at no
point are developers required to perform any access controls beyond deciding
whether or not their app-specific volume will be world-readable or private
(enforced internally through encryption).  This removes the need for developers
and users to coordinate with one another---Gaia ensures that applications' storage
interactions never interact, and ensures that users can only read one anothers'
data if they interact at all.

Gaia users are self-sufficient---there is no designated third party service that is
responsible for keeping the system alive, since users interact with their data
through device-hosted Gaia nodes.  However, users nevertheless need to recover
access to their data in the event they lose their computing devices.

To facilitate this, the configuration state for a user's Gaia node is replicated to \textit{all} of
the user's storage providers.  This state includes all app-specific public keys,
as well as all encrypted authentication tokens for their storage providers.

The configuration bundle is signed and encrypted with keys linked to
the user's identity on the SSI system's blockchain, so no matter which device(s) the user
uses to modify their configuration state, they will be able to always be able to
at least authenticate the externally-hosted data (even if they lose all of their
devices).  If the user changes their keys (i.e. in order to recover from device
loss), the configuration state is
automatically re-signed and re-encrypted by the Gaia node.

The only time a user directly interacts with a cryptographic key is when they
change the device(s) they use to interact with their data.
In our implementation, we facilitate this by
encoding an encrypted ``master'' private key as a 12-word pneumonic phrase, and derive keys
for signing name updates and for signing app-specific volume data
using a deterministic key-generation
algorithm~\cite{bip39}.  The encrypted private key is backed up to their email provider by
default.

\section{Syndicate: A Scalable Software-defined Storage System}

Syndicate is a scalable software-defined storage system meant for scientific
workloads.  Unlike Gaia, Syndicate is designed to provide shared volumes that
efficiently leverage CDNs for read loads and 
support I/O from a scalable number of concurrent users.  This makes
it ideal for sharing data across compute clusters, where the data sources and
sinks reside in different organizations.

\subsection{Motivation}

Science research is increasingly data-driven and increasingly distributed.
Researchers often share large datasets with other labs across the world and 
with the public.  As the cost of storage space becomes cheaper, scientists can
afford to generate and retain larger and larger amounts of data for the
indefinite future.

These trends create an interesting set of operation challenges:

\begin{itemize}
   \item How do scientists onboard new users and labs that use different
      technology stacks than their own?
   \item How do scientists keep legacy data-processing workflows running in the face
      of changing storage and compute systems?
   \item How do scientists take advantage of commodity storage and
      compute technologies without having to write a lot of bespoke code
      to do so?
   \item How do scientists enfoce data access and retention policies when the
      underlying storage substrate can be changed out from under them?
\end{itemize}

The standard practice today is messy.  Each time a lab wants to change its
storage system, it must re-work its workflows to be compatible.  This entails
more than patching the code to read and write data.  It also means changing their
operational practices for staging data for computations and changing the way they
share data internally and with other labs.

The recent ``containerized approach'' to using relocatable containers, VMs, and
SDNs to preserve the runtime environment for scientific workflows
is a step in the right direction.  However, this does not respect existing
organizational boundaries, since it leaves scientists with the difficulty of
copying their data into the new runtime and copying results back out.  These
extra steps are both burdensome and risky, since it places scientists in
the uncomfortable position of having to manually stage data for consumption and
manually copy results back into their lab's archival storage (running the risk
of interrupted data transfers, data corruption, data clobberring, and so on).

The reason scientists are put into this position is because scientific workflows
span multiple organizational boundaries.  Organizations include individual
scientists, research groups in the same lab, collaborative research groups who
work across multiple labs (including multiple universities, corporations, and
countries), and the general public.  Whenever a scientist in one organization
needs data in another organization, they need to manually copy it out into their
organization's storage (such as by downloading it, or by getting explicit access
permission).  At the same time, whenever a scientist needs to report the results
of their workflow to another organization, they have to replicate it in a place
where other organizations can read it.

There is a third challenge besides acquiring and replicating data:  external
organizations must be able to read and write data without being trusted to
enforce the rules for acquiring or replicating it.  For example, the students in
a university classroom must be able to acquire image data from telescopes, run
their lab experiments on it, and then store their results into their professor's
lab's cluster.  The university students constitute a separate organization from
both the astronomy lab that produced the data and the professor's lab.
Nevertheless, they must be able to interact with the data under a fixed set of
access semantics; otherwise the tools they would use would break.

\subsection{Gateway Types}

Syndicate accomodates cross-organizational data acquisition and
data replication by supplying specially-crafted gateways designed to make it
easy to share and store data.  An organization that wishes to share data with
another organization would encode its rules for allowing access into an
\emph{acquisition gateway} that takes care of indexing and exposing the data as
manifests and blocks.  An organization that wishes to store the results of
scientific computations would run a \emph{replica gateway} that enforces rules
that govern whether or not (and how) to store manifests and blocks within the
organization's storage systems.  Linking the two together are \emph{user
gateways} that expose the Syndicate-formatted data to scientific workflows in a
workflow-defiend manner, such as an externally-mounted filesystem within a
container.

\textbf{Acquisition gateways} (AGs) are gateways that connect to an externally-hosted
dataset and ``import'' its records into a Syndicate volume in a read-only
fashion.  It does so by crawling its backend dataset, and publishing metadata
for each (logical) record to the Syndicate MS.  Other gateways read the dataset
by first discovering the metadata, and then asking the AG for the manifest and
chunks (which it generates on-the-fly by fetching data from its backend
dataset).

\textbf{Replica gateways} (RGs) are gateways that connect to existing storage
systems.  They provide a read/write interface at the chunk granularity.  We have
implemented service drivers for Amazon S3~\cite{s3}, Dropbox~\cite{dropbox},
Google Drive~\cite{google-drive}, Amazon Glacier~\cite{amazon-glacier}, iRODS,
and local disk (for compatibility with NFS~\cite{nfs}, AFS~\cite{afs},
Ceph~\cite{ceph}, and other legacy distributed filesystems used today).

\textbf{User gateways} (UGs) are gateways that connect users and their workflows
to other gateways.  Each UG provides a different interface to workflows, subject
to their needs.  For example, we have implemented a UG that implements a
FUSE~\cite{fuse} filesystem, a UG that implements a RESTful~\cite{rest}
interface, a UG that implements a suite of UNIX-like shell utilities, and a UG
that implements a Hadoop filesystem~\cite{hadoop} backend.

Each organization runs the appropriate gateways on their computers depending on
how they wish to interact with the data.  This allows scientific workflows to
run across organizational boundaries in an automated fashion, allowing
scientists to \emph{independently} devise new workflows without incurring the
cost of coordinating with each lab.

For example, an astronomy lab would
run acquisition gateways to expose telescope images of earth.  They could stipulate rules
in its aggregation driver code that ensure that newly-generated images are only
readable to a privileged set of labs for a time (e.g. only labs in the same
country) before releasing them to the public.  Similarly, a meteorology
lab would run replica gateways to store data from trusted scientists, and
store them in a time-series fashion.  Unbeknownst to either lab, a scientist could
run a user gateway on her laptop and on her VMs that allowed her to read from both the
astronomy and meteorology labs' gateways and write data to both the meteorology
lab and to her Dropbox account to be shared with her collaborators.  By having
multiple types of gateway running in these specific roles, no coordination was
necessary between the astronomy and meteorology labs.

Syndicate allows operators to specify new gateway types at runtime, allowing
them to incrementally deploy and adapt the system to changing workloads.  Each
gateway's type is embedded in their certificate, so each gateway knows at all
times the network addresses and types of all other gateways in the volume.
This is useful for both scaling up the number of requests a gateway can handle,
and for creating distributed implementations of aggregation driver stages.  We
explore examples in Chapter~\ref{chap:applications}.

\subsection{Data Organization}

Unlike Gaia, each record in a Syndicate volume has its own manifest, and is
comprised of a variable number of blocks.  The block size is fixed for the
volume, but each volume can have its own block size.

Volumes in Syndicate can have arbitrarily many data records, and each data
record may have arbitrary sizes (i.e. made of arbitrarily many blocks).
Manifests, blocks, and certificates are all cacheable for
indefinite amounts of time, since Syndicate ensures that they are all immutable
(that is, they each receive new IDs in the system when their contents change).

Readers construct URLs to manifests, blocks, and certificates using their IDs to
ensure that any intermediate caches serve the right data.  Readers learn the IDs
directly from the MS, and use in-band hints to determine when their view of
these IDs is stale (as described in Chapter~\ref{chap:design_principles}).

\subsection{Data Flows}

Syndicate gateways route requests to one another based in part on what their
type is.  Specifically,
UGs initiate access flows to AGs and RGs to handle reads, but initiate mutate
flows only to RGs to handle writes.  AGs and RGs do not initiate any flows of
their own.

AGs are always the coordinators for the records they Publish.  They mark their
records as read-only, and will not participate in any mutate flows for them.

RGs load, store, and delete chunks in their underlying storage systems.  They do
not serve as coordinators.

When a UG wants to write to a record, it first replicates its chunks to all RGs in
the volume.  Once this succeeds, it sends its new manifest to the coordinator
gateway (also a UG), which updates the record's metadata on the MS to point to
the manifest the writer uploaded previously.  The coordinator then proceeds to
garbage-collect all unreferenced blocks and manifests, discussed below.

\subsubsection{Garbage Collection}

An interesting consequence of immutability is that writes to a record will cause
overwritten blocks and manifests to become unreferenced.  To prevent leaks, Syndicate's gateways
execute a distributed garbage-collection protocol to remove them.  The process
is asynchronous and tolerant of gateway failures.

When the coordinator of a record uploads new metadata to the MS, it includes a
vector of block IDs and the old manifest ID.  These are appended to a per-record
log in the MS.  Once the write completes, the coordinator asynchronously queries
the MS for the first $k$ entries in this log, constructs \texttt{delete}
requests for them, and sends the requests to the volume's replica gateways.
Once all replica gateways successfully acknowledge deletion, the coordinator
instructs the MS to remove the $k$ entries from the log.

\subsection{Metadata Service}

Syndicate's MS runs on top of a scalable NoSQL database.  In practice, our
deployments run within Google AppEngine~\cite{google-appengine} and
AppScale~\cite{appscale}, meaning that
Syndicate's metadata is hosted in either Cassandra~\cite{cassandra},
Hbase~\cite{hbase}, MySQL~\cite{mysql}, Hypertable~\cite{hypertable}, Megastore~\cite{megastore} or
Spanner~\cite{spanner}.  In all cases, writes to a single key are atomic, and
multi-key transactions are allowed provided that the set of keys is small (e.g.
five or less in the implementation).

The reason for this design choice is to make the MS deployment as automatable as
possible.  When running on Google Appengine, for example, deploying an MS from
scratch can be done simply by creating a new Appengine project and pushing the
code from the user's laptop to Google's servers.  No further maintenance or
infrastructure administration is required, beyond setting up billing.

\subsection{Programming Model}

Syndicate gateways implement HTTP servers internally to serve chunks to one
another.  Their driver programming model is inspired by fast CGI~\cite{fastcgi},
whereby the server spins up one or more long-lived "worker" subprocesses to
handle a particular kind of request (e.g. defined by a canonical HTTP path).
As such, a gateway's driver runs in a set of first-class processes, where each
process runs one aggregation driver stage, and the gateway forks and joins
individual processes in response to increased or decreased load.

Syndicate's driver model distinguishes between the \emph{logical} representation of a record,
the \emph{application} representation of the record, and the 
\emph{on-the-wire} representation of the record.  The logical representation is
the view of the data within a gateway (i.e. the ``narrow waist'' that connects
the application representation to the on-the-wire representation).  In this
representation, each record appears as a flat byte array (i.e. a file), with
fixed-sized blocks and one manifest.

Application-facing gateways (i.e. UGs in Syndicate) are free to represent data 
to the application in any way they want.  For example, an UG implementation
may represent a data record as a
SQL database.  Such a UG would require applications to interact with the data
via SQL commands.  The implementation would translate the commands into
\texttt{get}, \texttt{put}, and \texttt{delete} operations over the record's
blocks at the logical layer.  Syndicate implements a UG programming library and
SDK to allow developers to provide application-specific interfaces.

Syndicate's aggregation driver model also gives gateways the ability to
control a record's chunks' on-the-wire representation.  This allows the
developer to control how the networks that connect gateways view the data.  For
example, the developer can implement end-to-end encryption by encrypting and
decrypting chunks as they are transmitted and received, thereby hiding
data from the networks.  As another example, the developer can buffer and send
batches of chunks between gateways on-the-wire independently of the logical and
application representations.

A driver for a Syndicate gateway has three components:  a service-facing
component (implementing the service driver), and a two-part aggregation
component that translates between the logical layer and the on-the-wire layer
(as one module) and implements the application's end-to-end storage behavior (as
a separate module).

\subsubsection{On-the-wire Processing}

All gateway drivers implement a \texttt{serialize()} and \texttt{deserialize()}
method to translate a logical block or manifest to its on-the-wire
representation and back.  The \texttt{serialize()} method is called whenever the gateway sends
data or caches it to disk, and the \texttt{deserialize()} method is called
whenever the gateway receives data or loads it from its on-disk cache.

Unlike the remainder of a gateawy's methods, these methods are \emph{always}
invoked whenever a chunk is loaded or stored by the gateway.

\subsubsection{Acquisition Gateway Service Drivers}

The AG driver model is designed to handle datasets that can change from
external modifications.  For example, our iRODS AG driver
subscribes to the iRODS event queue, which allows it to get notified
when files it indexes change.  This allows it to push updates for them to the MS
in order to ensure that the state of the backend dataset is accurately reflected
by the volume.

\noindent{\textbf Aggregation Driver:} An AG only needs to implement the Publish stage of
the aggregation driver model, since it will never initiate access or mutate
flows.  Its Publish stage is implemented as a method that the AG repeatedly
calls.  It takes nothing as input, but outputs new record metadata and a
hint as to whether or not to create, update, or delete the record on the MS.  The
implementation is allowed to block the AG in the event that the backend dataset
has not changed.

\noindent{\textbf Service Driver:} When another gateway asks for a block from
one of the records, the AG forwards it to its service driver in order to fetch
the bytes from the dataset (the \texttt{read()} method).  The AG will
automatically generate manifests on request.

\subsubsection{User Gateway Drivers}

The UG driver model is designed to pull chunks from RGs and AGs, and push new
chunks to RGs.  Unlike the other gateways, the UG driver model gives developers
a chance to have the UG connect to one or more CDNs to fetch chunks.

\noindent{\textbf Aggregation Driver:}  The UG is mainly concerned with reading
data, and only allows the developer to customize the Discover, Acquire, and
Publish stages.  The UG itself handles communication with the MS to Discover new data,
but it lets the driver code decide whether or not a given access should contact
the MS.  This stage is implemented as a method that takes the record metadata as
input, and outputs a yes/no response as to whether or not to contact the MS.
This way, the driver can implement
whatever view of the data the application needs by ensuring that the UG
Discovers new data at the right times (but with the constraint of only being
able to present views of data as it had existed at some point in the past).

The UG driver model defines the Acquire stage as a method that takes some
metadata about the chunk to fetch as input, and returns as output a
URL that, when resolved by the UG, will return the particular chunk's data.
The Acquire stage may invoke the service driver (described below) to connect to
underlying network caches in-between upstream RGs and AGs, and may carry out any
pre-fetching in order to place the data such that the URL it generates will
resolve to the data.  As an optimization, the
UG supports a handful of widely-used protocols by default (HTTP, FTP, local
disk), so often times Acquire stage implementation only needs to
generate the appropriate URL.

% not implemented yet
The UG's Publish stage is invoked whenever the application either creates a
record or synchronizes its state.  The stage is defined as a
method that takes new record metadata as input, and outputs new record metadata for the UG to
send to the MS.  If the metadata is unchanged, then no information is sent to
the MS.  This not only allows the developer to control the circumstances under which
new data is exposed to the volume, but also gives the developer a chance to
carry out any side-effects of doing so (such as logging the creation or
modification of each record to a third party for later audits).

\noindent{\textbf Service Driver:}  The service driver in the UG is designed to
be used to fetch data that cannot be handled by one of the UG's built-in
protocol handlers.  In the rare case where the UG implementation is unable to carry out
a data transfer on its own, the the Acquire stage 
invokes the service driver to fetch the data, store it to local disk, and feed the UG a
\texttt{file://}-schemed URL that points to it.

To carry out a mutate flow, the UG serializes blocks and manifests and sends them to all RGs
in the volume.  The mutate flow succeeds only if all RGs
acknowledge successful replication.  Developers does not have the ability to
control when or how mutate flows are processed beyond controlling their
on-the-wire serialization.  Instead, we give developers the
ability to control how RGs handle chunks once they are received.

\subsubsection{Replica Gateway Drivers}

RGs allow the developer to customize how data will be stored.  RGs do not
initiate any access or mutate flows of their own, but instead participate in
flows initiated by UGs.  As such, the RG driver model complements the UG driver model---
it allows developers to customize the Push and Acquire stages.

\noindent{\textbf Aggregation Driver:}  The RG driver model gives the developer
the ability to load, store, or delete chunks.  It gives the driver code insights
as to whether or not a chunk is a block or a manifest, and which bytes in the
record it represents.  This gives the developer the ability to reason about how
individual chunks affect the view of the whole record.

The Push stage is defined as a method that takes the chunk and chunk metadata as
input, and returns success or failure.  Its responsibility is to make the chunk
persistent, such that any subsequently-executed Acquire stage \textit{from any
RG in the same volume} will successfully fetch the chunk data (barring network
errors).  The implementation is allowed to contact other RGs and their running
driver processes in order to make this guarantee (such as to implement a total
ordering on chunk writes).

The Acquire stage complements the Push stage.  It is defined as a method that
takes the chunk metadata as input and returns the previously-Pushed chunk as
output.

\noindent{\textbf Service Driver:}  The Acquire and Push stages each call into
the service driver to load, store, or delete the raw bytes.  The service driver
translates the chunk metadata into chunk-specific addresses, which it uses to
access or remove the data in a service-specific way.

\subsection{Administration}

Syndicate divides administrative responsibilities between volume owners and
gateway owners.  Each user that owns a gateway in a volume can control the
storage and aggregation driver code it runs.  This is necessary to ensure that
each organization retains the ability to control which code it runs.
In the UG case, this allows each
scientist to independently tailor their view of the data to their workflow.  In
the AG case, this allows labs to preserve how their data is presented to the
world even when the underlying dataset changes its data format or access
semantics.  In the RG case, this allows labs to preserve data availability and
serialization even when the underlying storage systems are changed out.
A volume owner retains the ability to unilaterally control all other fields of
the volume's certificate graph.

Administrating a gateway is similar to managing a \texttt{.ssh} directory.  As
long as a computer has the appropriate private keys, it can run the gateway.  This
allows the user to run a single logical gateway across as many computers as need
be, provided that the set of computers has the same network address (e.g. they
could be positioned behind a commodity HTTP load-balancer which has the gateway's network
address).

Volume administration is designed to be carried out from the volume owner's
personal device, and only their personal device.  The volume owner is not
required to trust a third-party service to execute the certificate graph update.
Instead, to propagate changes
to the certificate graph, the volume owner uses the Syndicate
administrative tool to first replicate the new, signed certificate graph to one or more
existing storage services that the gateways know how to access (e.g. the tool
can replicate the data to an HTTP-addressible cloud storage provider).  Once the
new certificate graph state is available, the tool contacts the MS and
each gateway via their certificate-listed network
addresses to instruct them to reload their views of the graph.  The tool includes the
certificate version vector information in the request, so the remote gateways will be able to
determine the freshness of the fetched certificate graph state in addition to
its authenticity.  If all gateways in the volume acknowledge success, then the
volume will have been reloaded by the time the next access or mutate flow
executes.  Even then, gateways will not participate in a flow unless they have
the latest view (and the MS will NACK messages from gateways if they do not
report the latest version).

The tool itself offers a simple set of CRUD commands for users, volumes, and
gateways, as well as a ``list'' command that can select objects by field value.
When combined with an SSI system, the tool does not require users to interact with
public keys at all (since each user, including the user that manages the MS,
registers their public keys under an easy-to-remember persistent name in the
SSI's blockchain).

Because Syndicate volumes are readable and writeable by many users, and because
a single MS can host many volumes, there additionally exists an ``admin''
organization that has the power to unilaterally alter the MS state.  Only an
admin user can create and delete users and change individual users' quotas.
The organization that pays the bills for the MS controls the admin user.

\section{Discussion}

Both Gaia and Syndicate minimize the marginal cost of adding support for
existing services by imposing a communication discipline between the services'
endpoints and the application (in the form of chunks and record-specific hints).
This keeps the service drivers isolated from both applications and higher-level
aggregation logic, so they can be reused in many contexts.  There is little
difference between their service driver models and implementations; in fact,
drivers from one system are easily ported to the other.

Gaia and Syndicate both minimize the marginal cost of adding support for new
storage semantics as well.  In Gaia's case, a user can alter their data's
storage semantics simply by (1) standing up a publicly-routable Gaia node that adds the new
feature, and (2) updating their routing information to send access and mutate
flows through it.  The process is analogous in
Syndicate:  a volume owner adds or updates an AG or RG to implement the new
functionality, and the UGs automatically take advantage of it.
Neither the applications nor the
storage systems need to be significantly modified to take advantage of the new
feature.

Minimizing the cost of cross-organizational coordination requires identifying
organizations by the network paths that data take when a volume's principals
read and wite it.  In Gaia's case, each organization is a (user,application) pairing, since
application state is only writable by the user that owns it and is only readable
by the users (s)he allows.  Gaia enables users to individually control how their
data is accessed without communicating with the application developer or other
users, simply by changing the code that executes in response to their queries.
At the same time, Gaia
frees developers from having to seek the users' permission to process
their data, simply because they never store it in the first place (and may even
be blocked from reading it).  Users and developers to not even need to be aware
of each others' existence for applications and user data to stay available; this
cannot be said of today's Web applications.

In Syndicate's case, an organization is any group of scientists
that interact with the same dataset.  The cross-organizational coordination
difficulties come from scientists trying to share data with one another.
Syndicate reduces the coordination costs between data publishers and data consumers by
interposing an AG.  This way, a data publishing group can store data however
they want as long as there exists an AG that can translate the data into the
formats required by the consumers.  Either the publishing group or the consuming
group can run the AG.  Once the AG driver code is written and published,
all consumers can get the same consistent view of the data and the same access
semantics without having to get the data producers to commit to a particular
publishing strategy.

A similar story exists for replicating data.  Either the data producer or data
consumer can stand up an RG to ingest the incoming data, but the presence of the
RG allows the producer and consumer to independently choose their data formats
and write semantics.  As long as the RG can do the proper translations, users
that write to the volume do not need to worry about the choices the data
recipients make (and vice versa for the producers).

The availability of a separate UG ensures that producers and consumers can keep
their applications forward-compatible with future AGs and RGs.  The UG provides
the application-expected interfaces, formats, and access semantics, so
programs and workflows written today can continue working even as Syndicate's
other gateways evolve.  This ensures that scientists who get their workflows
working with one UG can continue to run them, without having to worry about
changes to AG and RG deployments.

\subsection{Implementation}

Syndicate is implemented in 30,000 lines of C++ and 36,000 lines of Python 2.
Gaia is implemented in 14,000 lines of Python 2 (this count includes the Atlas
network implementation, but not the SSI system implementation that it uses to
identify zone file hashes).  The SSI system that Gaia relies on (Blockstack) is
implemented in 39,000 lines of Python 2 (13,000 lines implement the
blockchain indexer and name database, and 26,000 implement the client that
queries the indexer and sends transactions).

Both Gaia and Syndicate have read-write drivers for local disk, Amazon S3~\cite{s3}, 
Dropbox~\cite{dropbox}, Google Drive~\cite{gdrive}, and a Kademlia
DHT~\cite{kademlia}, as well as read-only drivers for HTTP, FTP, and WebDAV
resources.  Service drivers are written in Python 2 and are less than 200 lines of code
each.  Service drivers for Gaia are easily ported to Syndicate and vice versa.


\comment{

% --- notes

\section{Implementation Considerations}

By giving developers the freedom to implement storage processing at intermediate
points in the network, we enable them to design application storage that
respects each organization's hosting policies as data enters or leaves the
organization's doimain.  This is important for both scientific data and
decentralized applications, since they require coordination across
organizations.

\subsection{Example: Decentralized Document Editor}

For example, the user of a decentralized shared document editor
would want to ensure that other peoples' writes to her files are vetted
before being made externally visible.  To do so, she ensures that her volume's files
are coordinated by a single gateway running on her laptop.
Her volume's aggregation driver logic makes other
gateways buffer their Publish requests to one of a set of network-addressable
message queues, so that when
her laptop comes online, it will dequeue them and present them to her for
explicit acknowledgement.  This user works with confidential files at work and
personal files at home, so the Publish queue for work must be addressable only
on her employer's servers (the queue for her personal files may run anywhere and
be globally addressable).

Implementing this application without SDS would be
challenging, since the application would need to be aware of the fact
that there is a choice of which of the queues to use, and that the choice depends on
the file being edited.  With SDS, the
user only needs to load an aggregation driver for her volume that routes 
and mutate flows to the right queue.  Any application that can access her volume
automatically gets the required storage semantics.

\subsection{Example: Sharing an HPC Cluster}

As another example, a PI maintains an HPC cluster that her lab needs to share
with other collaborators.  Each collaborator hosts their datasets locally, and
needs to write back their changes from the HPC job.  The HPC cluster has a
``staging space'' and where collaborators may write their input data.  The
cluster itself has an ``output space'' where nodes write the results of their
computations.

Getting data to flow from a collaborator's lab to the HPC cluster and back is
straightforward with SDS.  The PI creates a volume for the staging space data
and a volume for the output space data.  Both volumes' aggregation drivers know
how to identify which collaborator issued which Publish or sent which chunk.
Each HPC node is given a read-only gateway to the staging space and a write-only
gateway to the output space.  Each collaborator is given a gateway for writing
data into the staging space, and a gateway for receiving data out of the output
space.  The aggregation driver logic for both volumes simply tracks which chunk
came from which collaborator, so the output space writes will be sent to the
right collaborator's gateways.

The result is that from the collaborator's perspective, submitting a job to the HPC
cluster is just a matter of writing the input into the local staging space
gateway, and later reading the output from the local output space gateway.  The
collaborators gets to choose which application-facing interfaces their local
gateways run, and get to use a reusable set of service libraries to push and pull
chunks into their storage providers and local storage facilities.
Without SDS, the PI would need to implement a job submission middleware that
handled collaborator-required application interfaces, collaborator-chosen
storage, and collaborator authentication.

\subsection{Gateway Roles}

Our examples show that gateways will assume specialized roles based on
which services they interact with.
While the SDS system gives the volume owner free reign over what each gateway is
capable of doing, in practice there are three common gateway roles.
They are:

\begin{itemize}
    \item A \textbf{replica gateway} is a gateway that runs service drivers for
cloud storage providers.  They do \emph{not} implement a storage programming
interface; they only interact with other gateways.  Moreover, these gateways are
not coordinators for any data (i.e. they never Publish).  Instead, other gateways send
them chunks and fetch them back.
    \item An \textbf{acquisition gateway} is a gateway that runs service drivers
for external data sets providers.  They do \emph{not} implement a storage
programming interface, but instead expose external data as read-only SDS data that happens
to be ``owned'' by the gateway's operator.  As such, these gateways are always
the coordinators for the data they expose.  However, they do not accept chunks.
    \item A \textbf{user gateway} is a gateway that runs service drivers for
CDNs.  Unlike the other two, these gateways \emph{do} implement a storage
programming interface, such as a filesystem or an HTTP RESTful API.
User gateways are the coordinators for the data created by the user that runs
them.  The name for this gateway is apt, since application users run
these gateways so their clients can interact with their data.
\end{itemize}

In our experience, we have found that these three roles are sufficient to
express complex application-specific storage semantics in non-trivial
applications.  Other roles are allowed by the SDS design parameters, but we have
not explored them.

In practice, the volume owner assigns each user-owned device a
user gateway, and gives it a service driver for fetching chunks via a 3rd party
CDN.  She creates one or more replica gateways owned by her SDS user account,
which will run service drivers for her volume's cloud storage providers.  These
replica gateways will simply load and store chunks to and from these services.
If the application needs access to
third party datasets, she will additionally create acquisition gateways for
herself that will index the 3rd party datasets (via dataset-specific service
drivers) and Publish SDS data records for them that map onto the dataset.

}
